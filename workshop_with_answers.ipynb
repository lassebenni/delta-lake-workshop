{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the SparkSession\n",
    "\n",
    "Run the cells below to get a SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.8.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store data in delta format\n",
    "\n",
    "The data we use is the COVID-19 daily report data from Johns Hopkins university. It's already download to `/home/jovyan/COVID-19/csse_covid_19_data/csse_covid_19_daily_reports/` as CSV files, with the filename formatted as the date in American format (mm-dd-yyyy). For now we will only load the April 2021 data.\n",
    "\n",
    "After loading the CSV data, you need to write it in the delta format. Use `/tmp/deltalake/` to avoid permission issues.\n",
    "\n",
    "- [PySpark cheat sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf)\n",
    "- [Spark SQL documentation](https://spark.apache.org/docs/latest/api/python/reference/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").options(header='True', inferSchema='True', delimiter=',').load(\"/home/jovyan/COVID-19/csse_covid_19_data/csse_covid_19_daily_reports/04-*-2021.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").save(\"/tmp/deltalake/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta lake writes parquet files to location on disk. Can you find those files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /tmp/deltalake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data in delta format\n",
    "\n",
    "After storing the data in delta format you can now read it. Try showing the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/tmp/deltalake/\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up the data\n",
    "\n",
    "We use the `FIPS`, `Admin2`, `Province_State`, `Country_Region` and `Last_Update` as the unique key of this dataset. For ease of use later on we want to clean up the nulls in `FIPS`, `Admin2` and `Province_State` column.\n",
    "\n",
    "Do this using DeltaTable class with the update method. Replace nulls for strings with an empty string (`''`) and for numbers with `0`.\n",
    "\n",
    "Hint: you can check for nulls with the `isNull()` method of `f.col()`.\n",
    "\n",
    "- [Documentation of DeltaTable](https://docs.delta.io/latest/api/python/index.html).\n",
    "- [Documentation of Spark SQL Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, '/tmp/deltalake')\n",
    "\n",
    "deltaTable.update(f.col(\"FIPS\").isNull(), { \"FIPS\": \"0\" } ) \n",
    "deltaTable.update(f.col(\"Admin2\").isNull(), { \"Admin2\": \"''\" } ) \n",
    "deltaTable.update(f.col(\"Province_State\").isNull(), { \"Province_State\": \"''\" } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta Lake keeps a history of you data. You can check it out with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_history_df = deltaTable.history() \n",
    "table_history_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsert new data\n",
    "\n",
    "The Confirmed cases for Flevoland in the Netherlands turned out to be different. We've received a new file with the fix, it's stored in `/home/jovyan/delta-lake-workshop/files/04-07-2021.csv`.\n",
    "\n",
    "- Clean the new data with the same rules as you cleaned the existing data in the deltalake (replace null with `''` or `0`).\n",
    "- Upsert the new data with a merge into the deltalake. You can find an example [here](https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge). Remember the columns that make up the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updates_df = spark.read.format(\"csv\").options(header='True', inferSchema='True', delimiter=',').load(\"/home/jovyan/delta-lake-workshop/files/04-07-2021.csv\")\n",
    "updates_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updates_df = updates_df.withColumn('FIPS',\n",
    "    f.when(f.col('FIPS').isNull(),0).\n",
    "    otherwise(f.col('FIPS'))) \\\n",
    "    .withColumn('Admin2',\n",
    "    f.when(f.col('Admin2').isNull(),'').\n",
    "    otherwise(f.col('Admin2'))) \\\n",
    "    .withColumn('Province_State',\n",
    "    f.when(f.col('Province_State').isNull(),'').\n",
    "    otherwise(f.col('Province_State')))\n",
    "updates_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.alias(\"covid19\").merge(\n",
    "    updates_df.alias(\"updates\"),\n",
    "    \"covid19.FIPS = updates.FIPS AND \\\n",
    "    covid19.Admin2 = updates.Admin2 AND \\\n",
    "    covid19.Province_State = updates.Province_State AND \\\n",
    "    covid19.Country_Region = updates.Country_Region AND \\\n",
    "    covid19.Last_Update = updates.Last_Update\") \\\n",
    "  .whenMatchedUpdateAll() \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time travel\n",
    "In a data warehouse (or lakehouse) you often want to be able to time travel, so you can see what the world looked like at a certain moment in time. This feature is very useful for data rollbacks, auditing, and reproducing reports and analytics at any point in time.\n",
    "\n",
    "Since history is kept, this feature is supported out of the box. You can find examples on how to access the history [here](https://docs.delta.io/latest/delta-batch.html#query-an-older-snapshot-of-a-table-time-travel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_merge_df = spark.read.format(\"delta\").option(\"versionAsOf\", 3).load(\"/tmp/deltalake/\")\n",
    "pre_merge_df.filter(pre_merge_df[\"Province_State\"]=='Flevoland').orderBy(pre_merge_df['Last_Update']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df[\"Province_State\"]=='Flevoland').orderBy(df['Last_Update']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Delta Lake: schema updates\n",
    "\n",
    "We didn't think you would get here already, but if you're up for it you can take it one step further.\n",
    "\n",
    "Remember that we only put April 2021 in our delta lake? The reason for this was that the data set has had quite a bit of schema drift in the last 1+ year. Delta Lake can deal with this, either manually (meh) or automatically (yay). Check out the examples [here](https://docs.delta.io/latest/delta-batch.html#update-table-schema).\n",
    "\n",
    "We haven't tried this feature yet, so we're curious what your solution is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
